[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.aiyoggle.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["aiyoggle"],"categories":null,"content":"Ziheng Zhang is a master student at the School of Information Science and Techology, ShanghaiTech University. His research insterests include computer vision and deep learning. Before coming to ShanghaiTech University, he receives his BSc. Degree from Xidian University, Xi`An, China. He is currently looking for research assistant position (Jul.2019-Jan.2020) and doctoral program (Spring, 2020) in computer science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3f9c9fc85105624471026a7930ee6b37","permalink":"https://www.aiyoggle.me/authors/aiyoggle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aiyoggle/","section":"authors","summary":"Ziheng Zhang is a master student at the School of Information Science and Techology, ShanghaiTech University. His research insterests include computer vision and deep learning. Before coming to ShanghaiTech University, he receives his BSc. Degree from Xidian University, Xi`An, China. He is currently looking for research assistant position (Jul.2019-Jan.2020) and doctoral program (Spring, 2020) in computer science.","tags":null,"title":"Ziheng Zhang","type":"authors"},{"authors":["Ziheng Zhang*","Zhengxin Li*","Ning Bi","Jia Zheng","Jinlei Wang","Kun Huang","Weixin Luo","Yanyu Xu","Shenghua Gao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"48229302da089f67a475a8b7fc02b862","permalink":"https://www.aiyoggle.me/publication/ppgnet-cvpr19/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/ppgnet-cvpr19/","section":"publication","summary":"In this paper, we present a novel framework to detect line segments in man-made environments. Specifically, we propose to describe junctions, line segments and relationships between them with a simple graph, which is more structured and informative than end-point representation used in existing line segment detection methods. In order to extract a line segment graph from an image, we further introduce the PPGNet, a convolutional neural network that directly infers a graph from an image. We evaluate our method on published benchmarks including York Urban and Wireframe datasets. The results demonstrate that our method achieves satisfactory performance and generalizes well on all the benchmarks. The source code of our work is available at https://github.com/svip-lab/PPGNet.","tags":["Ling Segment Detection","Graph","CNN"],"title":"PPGNet: Learning Point-Pair Graph for Line Segment Detection","type":"publication"},{"authors":["Ziheng Zhang","Anpei Chen","Ling Xie","Jingyi Yu","Shenghua Gao"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"1809580b29d64690fea783723b668055","permalink":"https://www.aiyoggle.me/publication/sem-dist-acmmm19/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/sem-dist-acmmm19/","section":"publication","summary":"In this work, we demonstrate yet another approach to tackle the amodal segmentation problem. Specifically, we first introduce a new representation, namely a semantics-aware distance map (sem-dist map), to serve as our target for amodal segmentation instead of the commonly used masks and heatmaps. The sem-dist map is a kind of level-set representation, of which the different regions of an object are placed into different levels on the map according to their visibility. It is a natural extension of masks and heatmaps, where modal, amodal segmentation, as well as depth order information, are all well-described. Then we also introduce a novel convolutional neural network (CNN) architecture, which we refer to as semantic layering network, to estimate sem-dist maps layer by layer, from the global-level to the instance-level, for all objects in an image. Extensive experiments on the COCOA and D2SA datasets have demonstrated that our framework can predict amodal segmentation, occlusion and depth order with state-of-the-art performance.","tags":["Amodal Segmentation","CNN"],"title":"Learning Semantics-aware Distance Map with Semantics Layering Network for Amodal Instance Segmentation","type":"publication"},{"authors":["Yanyu Xu","Zhixin Piao","Wen Liu","Ziheng Zhang","Jingyi Yu","Shenghua Gao"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"577b6f23d0b80cf6c72fd116231ac065","permalink":"https://www.aiyoggle.me/publication/sunnet-bmvc19/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/sunnet-bmvc19/","section":"publication","summary":"This paper presents a novel Separation-and-UnioN Network (SUNNet) for simultaneous human parsing and pose estimation. Our SUNNet consists of two stages: feature separation and feature union. In feature separation stage, we leverage a common feature extractor to implicitly encode the correlation between human parsing and pose estimation, meanwhile task-specific feature extractors are designed to extract the features for each task. By combining the task-specific features with common features with a feature consolidation module in a coarse-to-fine manner, we can get the initial prediction for parsing and 2D pose estimation; In feature union stage, we refine the initial prediction by explicitly leveraging the features from parallel task to predict the kernels' receptive fields in a convolutional neural network. We further propose a leverage a 3D human body reconstructed from the image to facilitate these tasks. Extensive experiments demonstrate the effectiveness of our SUNNet model for human body configuration analysis.","tags":["Human Parsing","Human Pose Estimation","CNN"],"title":"SUNNet: A Novel Framework for Simultaneous Human Parsing and Pose Estimation","type":"publication"},{"authors":["Anpei Chen","Zhang Chen","Guli Zhang","Ziheng Zhang","Kenny Mitchell","Jingyi Yu"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"f1da8cad2d25e13265a6db6e72792dad","permalink":"https://www.aiyoggle.me/publication/photo-arxiv/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/photo-arxiv/","section":"publication","summary":"We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 20K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions.","tags":["Face Synthesis","CNN","GAN"],"title":"Photo-Realistic Facial Details Synthesis from Single Image","type":"publication"},{"authors":["Dongze Lian*","Ziheng Zhang*","Weixin Luo","Lina Hu","Minye Wu","Zechao Li","Jingyi Yu","Shenghua Gao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"bcc9da540eb8da4ae0e4bf5d785b08e6","permalink":"https://www.aiyoggle.me/publication/gaze-aaai19/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/gaze-aaai19/","section":"publication","summary":"This paper tackles RGBD based gaze estimation with Convolutional Neural Networks (CNNs). Specifically, we propose to decompose gaze point estimation into eyeball pose, head pose, and 3D eye position estimation. Compared with RGB image-based gaze tracking, having depth modality helps to facilitate head pose estimation and 3D eye position estimation. The captured depth image, however, usually contains noise and black holes which noticeably hamper gaze tracking. Thus we propose a CNN-based multi-task learning framework to simultaneously refine depth images and predict gaze points. We utilize a generator network for depth image generation with a Generative Neural Network (GAN), where the generator network is partially shared by both the gaze tracking network and GAN-based depth synthesizing. By optimizing the whole network simultaneously, depth image synthesis improves gaze point estimation and vice versa. Since the only existing RGBD dataset (EYEDIAP) is too small, we build a large-scale RGBD gaze tracking dataset for performance evaluation. As far as we know, it is the largest RGBD gaze dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the EYEDIAP dataset.","tags":["Gaze Estimation","CNN"],"title":"RGBD Based Gaze Estimation via Multi-task CNN","type":"publication"},{"authors":["Ziheng Zhang*","Yanyu Xu*","Jingyi Yu","Shenghua Gao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"66a3f009c86a8a405cf3ea89400e3648","permalink":"https://www.aiyoggle.me/publication/saliency-eccv18/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/saliency-eccv18/","section":"publication","summary":"This paper presents a novel spherical convolutional neural network based scheme for saliency detection for 360 videos. Specifically, in our spherical convolution neural network definition, kernel is defined on a spherical crown, and the convolution involves the rotation of the kernel along the sphere. Considering that the 360 videos are usually stored with equirectangular panorama, we propose to implement the spherical convolution on panorama by stretching and rotating the kernel based on the location of patch to be convolved. Compared with existing spherical convolution, our definition has the parameter sharing property, which would greatly reduce the parameters to be learned. We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net. To validate our approach, we construct a large-scale 360 videos saliency detection benchmark that consists of 104 360 videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our spherical U-net for 360 video saliency detection.","tags":["Saliency Detection","Spherical Convolution","CNN"],"title":"Saliency Detection in 360° Videos","type":"publication"},{"authors":["Ziheng Zhang","Jia Zheng","Shenghua Gao","Yi Ma"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"33062a82200e7d5f564887d5d04daed5","permalink":"https://www.aiyoggle.me/publication/webvision-cvpr17/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/webvision-cvpr17/","section":"publication","summary":"This report aims to study how to train a deep learning based classifier when only large scale noisy dataset is available. In order to overcome dataset noise, a series of training as well as testing methods are proposed, including bootstrapping method and ensemble method. Result shows that proposed methods achieve remarkable performance","tags":["WebVision","CNN"],"title":"2nd place in WebVision Challenge on Image Classification Task (Top University Team)","type":"publication"}]